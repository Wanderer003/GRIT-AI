{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWO4Tuh1cLc/ZQz08njRS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subhutii/GRIT-AI/blob/main/KeyPhrase_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzJ0EdM612WP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename=\"韓国から中国まとめ.csv\"\n",
        "df = pd.read_csv(filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_c = df.loc[:,[\"Country\"]]\n",
        "# print(df_c)\n",
        "counry=df_c[~df_c.duplicated()]\n",
        "\n",
        "cnt = df['Country'].str.split(',')\n",
        "Country = []\n",
        "\n",
        "for i in cnt:\n",
        "    if i not in Country:\n",
        "        Country.append(i)"
      ],
      "metadata": {
        "id": "JT4nJFwI19F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "LTq1nAk92DUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#国名ファイル用　日本語　→　英語\n",
        "from googletrans import Translator\n",
        " \n",
        "tr = Translator()\n",
        "CountryText=[]\n",
        "for i in range(len(Country)):\n",
        "      result=[i,Country[i]]\n",
        "      countryText=\"\".join(Country[i])\n",
        "      result = tr.translate(countryText, src=\"ja\", dest=\"en\").text\n",
        "      CountryText.append(result)\n",
        "\n",
        "print(CountryText)"
      ],
      "metadata": {
        "id": "ERbHgaXc2GnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Driveとマウントする必要がある\n",
        "\n",
        "**国とTextのみのファイル出力**"
      ],
      "metadata": {
        "id": "jLvHnIBV2JUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# CSVファイルの保管先\n",
        "csv_path = \"/content/drive/MyDrive/out/\"\n",
        "\n",
        "for i in range(len(Country)):\n",
        "  rm_Country=[]\n",
        "  # CSVファイル名＝国名\n",
        "  countStr= \"\".join(Country[i])\n",
        "  csv_name = countStr+(\".csv\")\n",
        "\n",
        "  f = open(csv_path+csv_name, 'w',encoding='utf-8_sig',newline='')\n",
        "  writer = csv.writer(f)\n",
        "\n",
        "\n",
        "  #国ごとにText抽出\n",
        "  dfS=df[df['Country'].isin(Country[i])]\n",
        "  text = dfS['Text'].str.split(',')\n",
        "  for i in text:\n",
        "    # print(i)\n",
        "    rm_Country=[i]\n",
        "    writer.writerow(rm_Country)\n",
        "  f.close()\n"
      ],
      "metadata": {
        "id": "MQRGj4DC2N6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**出力**"
      ],
      "metadata": {
        "id": "tVdWQv9K2Tnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 国ごとのtext読み込み\n",
        "# CSVファイルの保管先\n",
        "csv_path = \"/content/drive/MyDrive/out/\"\n",
        "\n",
        "for i in range(len(Country)):\n",
        "  rm_result=[]\n",
        "  # CSVファイル名＝国名\n",
        "  countStr= \"\".join(Country[i])\n",
        "  csv_name = countStr+(\".csv\")\n",
        "\n",
        "  with open(csv_path+csv_name, newline='') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "    print(lines)"
      ],
      "metadata": {
        "id": "j_jhT8uu2SEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "3XNfTu8u2eAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel > /dev/null 2>&1\n",
        "!pip install -U spacy > /dev/null 2>&1\n",
        "!python -m spacy download ja_core_news_sm > /dev/null 2>&1\n",
        "!pip install neologdn > /dev/null 2>&1\n",
        "!pip install emoji --upgrade > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "srP8ZUd42gRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import emoji\n",
        "import numpy as np\n",
        "import neologdn\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "import spacy\n",
        "from spacy.lang.ja import Japanese, DetailedToken\n",
        "from textacy import extract\n",
        "import ginza # 動作モード切替のため"
      ],
      "metadata": {
        "id": "lAJwGdwI2o7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import re\n",
        "\n",
        "def remove_emoji(text):\n",
        "   '''\n",
        "   remove all of emojis from text\n",
        "   '''\n",
        "   text=  emoji.demojize(text)\n",
        "   text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n",
        "\n",
        "   return text"
      ],
      "metadata": {
        "id": "as1a0hWL2rkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "def preprocess(x):\n",
        "  x=remove_emoji(x)\n",
        "  #絵文字の除去\n",
        "  emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"]+\",\n",
        "            flags=re.UNICODE,\n",
        "        )\n",
        "\n",
        "  x = emoji_pattern.sub(r\"\", x)\n",
        "\n",
        "  x = neologdn.normalize(x)\n",
        "  x = x.lower()\n",
        "  x = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", x)#URLの除去\n",
        "  x = re.sub(r\"[!\\?%-/:-@[-`{-~]\", r\" \", x)# 半角記号の置換\n",
        "  x = re.sub(\"[■-♯]\", \"\", x)# 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
        "  x = re.sub(r\"(\\d)([,.])(\\d+)\", \"\\1\\3\", x)#桁区切り (+ 小数点) 除去\n",
        "  # x = re.sub(r\"\\d+\", \"0\", x)#数字を0に置換\n",
        "  # x = re.sub(r\"\\d+\",\"\" ,x)#数字除去\n",
        "  x = re.sub(r'[a-zA-Z0-9]', '', x)#英数字除去\n",
        "  x = re.sub(r\"・\", \", \", x)#・→，\n",
        "  x = re.sub(r\"[\\(\\)「」【】]\", \"\", x)# 括弧削除（まだ消すやつある）\n",
        "  x = re.sub(r\"[\\！\\？＝＋－\\+\\*\\-\\&\\(\\)\\.\\,/=\\-，．。、\\ ￥’”→↓←↑…「」（）『 \\・～『』・；：⚫※％]\",\"\", x)\n",
        "\n",
        "  \n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "bP9upZWd2vgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract後の可視化\n",
        "def decompose_keyterms(keyterm_list: List[str]) -> Tuple:\n",
        "    terms = [el[0] for el in keyterm_list]\n",
        "    scores = [el[1] for el in keyterm_list]\n",
        "    return terms, scores\n",
        "\n",
        "#トークン化\n",
        "def create_doc(text):\n",
        "  nlp = spacy.load('ja_ginza')\n",
        "  mode=\"C\"\n",
        "  ginza.set_split_mode(nlp, mode) # モード切替\n",
        "  doc = nlp(text)\n",
        "  return doc\n",
        "\n"
      ],
      "metadata": {
        "id": "J_Jq9siW2wnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#textrank\n",
        "def textrank_algo(doc):\n",
        "    include_pos=[\"NOUN\", \"PROPN\", \"ADJ\"]#英語の品詞タグ名詞形容詞動詞，\n",
        "    textrank = extract.keyterms.textrank(doc, normalize=\"lower\",include_pos=include_pos,topn=30)\n",
        "    terms_textrank, scores_textrank = decompose_keyterms(textrank)\n",
        "\n",
        "    return terms_textrank, scores_textrank\n",
        "\n",
        "#PositionRank\n",
        "# def PositionRank_algo(doc):\n",
        "#     include_pos=[\"NOUN\", \"PROPN\", \"ADJ\"]#英語の品詞タグ名詞形容詞動詞，\n",
        "#     PositionRan = extract.keyterms.textrank(doc, normalize=\"lemma\",include_pos=None,window_size=10, edge_weighting=\"count\", position_bias=True,topn=20)\n",
        "#     terms_PositionRank, scores_PositionRank = decompose_keyterms(PositionRan)\n",
        "\n",
        "#     return terms_PositionRank, scores_PositionRank\n",
        "\n",
        "#yake\n",
        "def yake_algo(doc):\n",
        "    include_pos=[\"NOUN\", \"PROPN\", \"ADJ\"]#英語の品詞タグ名詞形容詞動詞，\n",
        "    yake = extract.keyterms.yake(doc, normalize=\"lower\",include_pos=include_pos,ngrams=(2, 3),topn=30)\n",
        "    terms_yake, scores_yake = decompose_keyterms(yake)\n",
        "\n",
        "    return terms_yake, scores_yake\n",
        "\n",
        "#scake\n",
        "def scake_algo(doc):\n",
        "    scake = extract.keyterms.scake(doc, normalize=\"lemma\")\n",
        "    terms_scake, scores_scake = decompose_keyterms(scake)\n",
        "\n",
        "    return terms_scake, scores_scake\n",
        "\n",
        "#sgrank\n",
        "def sgrank_algo(doc):\n",
        "    include_pos=[\"NOUN\", \"PROPN\", \"ADJ\"]#英語の品詞タグ名詞形容詞動詞，\n",
        "    sgrank = extract.keyterms.sgrank(doc, normalize=\"lower\",include_pos=include_pos,ngrams=(2, 3),topn=30)\n",
        "    terms_sgrank, scores_sgrank = decompose_keyterms(sgrank)\n",
        "    return terms_sgrank, scores_sgrank"
      ],
      "metadata": {
        "id": "VcGpD5XP2wit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#品詞でフィルタリング\n",
        "def preprocessPOS(text):\n",
        "  doc = create_doc(text)\n",
        "\n",
        "  #英語の品詞タグでストップワード作成\n",
        "  rmPOS=[\"PUNCT\", \"NUM\", \"SYM\",\"PRON\",\"INTJ\",\"SCONJ\",\"AUX\",\"PART\",\"CCONJ\",\"ADV\",\"ADP\",\"SPACE\",\"VERB\",\"DET\"]\n",
        "  filtere = [rmtoken.text for rmtoken in doc if rmtoken.pos_ in rmPOS]\n",
        "  # print(filtere)\n",
        "  #日本の品詞タグでストップワード作成\n",
        "  stopTag=[\"感動詞-フィラー\",\"感動詞-一般\",\"補助記号-句点\",\"補助記号-一般\",\"補助記号-読点\",\"記号-一般\",\"記号-文字\",\"名詞-数詞\",\"名詞-普通名詞-助数詞可能\",\"\t名詞-普通名詞-副詞可能\",\"代名詞\",\"接尾辞-名詞的-助数詞\",\"接尾辞\",\"接尾辞-名詞的-一般\",\"接尾辞-名詞的-副詞可能\",\"接尾辞-名詞的-サ変可能\",\"接頭辞\",\"助動詞\",\"助詞-係助詞\",\"助詞-格助詞\",\"助詞-終助詞\",\"助詞-副助詞\",\"助詞-準体助詞\",\"副詞\",\"連体詞\"]\n",
        "  filter1 = [token.text for token in doc if token.tag_ in stopTag]\n",
        "  # print(filter1)\n",
        "  #ストップワード結合\n",
        "  filtere_all = filtere+filter1\n",
        "\n",
        "  filtered_doc = [token.text for token in doc if not token.text in filtere_all]\n",
        "\n",
        "  Str = \" \".join(filtered_doc)# list -> str\n",
        "  \n",
        "  return Str"
      ],
      "metadata": {
        "id": "6Ti8FNKI2wf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StopWord**"
      ],
      "metadata": {
        "id": "cshe-lrQ29Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_text():\n",
        "  stopword_file = open(\"Newstopwords.txt\", \"r\")\n",
        "  stopword = []\n",
        "  w=stopword_file.readlines()\n",
        "  # print(w)\n",
        "  for line in w:\n",
        "    if line != '\\n':\n",
        "      stopword.append(str(line.strip()))\n",
        "\n",
        "  return stopword"
      ],
      "metadata": {
        "id": "3V5xyaah2waM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_stopword(x):\n",
        "  sl=stopwords_text()\n",
        "  result=[]\n",
        "\n",
        "  for token in x.split( ):\n",
        "    if not token in sl:\n",
        "      result.append(token)\n",
        "  w=set(result)\n",
        "  strResult=  \" \".join(w)# list -> str\n",
        "\n",
        "  return strResult\n"
      ],
      "metadata": {
        "id": "pK7wIiZv3HXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**csvまとめて分割 ＆ キーフレーズ抽出**"
      ],
      "metadata": {
        "id": "IHuhXAsr3NlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# 呼び出し・結果\n",
        "\n",
        "# CSVファイルの保管先\n",
        "csv_path1 = \"/content/drive/MyDrive/out/TextRank/\"\n",
        "csv_path2 = \"/content/drive/MyDrive/out/sgRank/\"\n",
        "csv_path3 = \"/content/drive/MyDrive/out/YAKE/\"\n",
        "\n",
        "# CSVファイルの保管先\n",
        "op_csv='/content/drive/MyDrive/out/'\n",
        "\n",
        "for i in range(len(Country)):\n",
        "  # CSVファイル名＝国名\n",
        "  c_name=\"\".join(CountryText[i])\n",
        "  countStr= \"\".join(Country[i])\n",
        "\n",
        "  with open(op_csv+countStr+(\".csv\"), newline='') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  chunks = [' '.join(lines[i:i+95]) for i in range(0, len(lines), 95)]\n",
        "\n",
        "  fTextRank = open(csv_path1+c_name+(\".csv\"), 'w',encoding='utf-8_sig',newline='')\n",
        "  writer_textRank = csv.writer(fTextRank)\n",
        "\n",
        "  fYAKE = open(csv_path2+c_name+(\".csv\"), 'w',encoding='utf-8_sig',newline='')\n",
        "  writer_yake = csv.writer(fYAKE)\n",
        "\n",
        "  fsgRank = open(csv_path3+c_name+(\".csv\"), 'w',encoding='utf-8_sig',newline='')\n",
        "  writer_sgRank = csv.writer(fsgRank)\n",
        "\n",
        "  textRank=[]\n",
        "  yake=[]\n",
        "  sgRank=[]\n",
        "  PositionRank=[]\n",
        "  \n",
        "  for chunk in chunks:\n",
        "    # rmPOS=preprocessPOS(chunk)\n",
        "    # pre = preprocess(rmPOS)\n",
        "    # doc = create_doc(pre)\n",
        "\n",
        "    rmPOS=\"\".join(chunk)\n",
        "    pre = preprocess(rmPOS)\n",
        "    doc = create_doc(pre)\n",
        "\n",
        "    terms_textrank, score_textrank = textrank_algo(doc)\n",
        "    terms_yake, score_yake = yake_algo(doc)\n",
        "    # terms_scake, score_scake = scake_algo(doc)\n",
        "    terms_sgrank, scores_sgrank = sgrank_algo(doc)\n",
        "    # terms_PositionRank, scores_PositionRank = PositionRank_algo(doc)\n",
        "\n",
        "    textRank+=terms_textrank\n",
        "    yake+=terms_yake\n",
        "    sgRank+=terms_sgrank\n",
        "    # PositionRank+=terms_PositionRank\n",
        "\n",
        "    setResult1=set(textRank)\n",
        "    result1=[]\n",
        "    for a in setResult1:\n",
        "      rm=rm_stopword(a)\n",
        "      result1+=[rm]\n",
        "\n",
        "    setResult2=set(yake)\n",
        "    result2=[]\n",
        "    for j in setResult2:\n",
        "      rm2=rm_stopword(j)\n",
        "      result2+=[rm2]\n",
        "\n",
        "    setResult3=set(sgRank)\n",
        "    result3=[]\n",
        "    for k in setResult3:\n",
        "      rm3=rm_stopword(k)\n",
        "      result3+=[rm3]\n",
        "      \n",
        "    result1=list(filter(None, set(result1)))\n",
        "    result2=list(filter(None, set(result2)))\n",
        "    result3=list(filter(None, set(result3)))\n",
        "\n",
        "    for t in result1:\n",
        "      t1=[countStr,t]\n",
        "      writer_textRank.writerow(t1)\n",
        "\n",
        "    for y in result2:\n",
        "      y1=[countStr,y]\n",
        "      writer_yake.writerow(y1)\n",
        "\n",
        "    for s in result3:\n",
        "      s1=[countStr,s]\n",
        "      writer_sgRank.writerow(s1)\n",
        "\n",
        "  fTextRank.close()\n",
        "  fYAKE.close()\n",
        "  fsgRank.close()\n",
        "\n",
        "\n",
        "  print(\"textrank \"+countStr, result1)\n",
        "  print(\"yake \"+countStr,result2)\n",
        "  print(\"sgrank \"+countStr,result3)\n",
        "# print(\"PositionRank \",PositionRank)"
      ],
      "metadata": {
        "id": "k0VCxkBA3HTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spacy**"
      ],
      "metadata": {
        "id": "jFrAin8w3sZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy\n",
        "op_csv='/content/drive/MyDrive/out/'\n",
        "countStr=\"オーストラリア\"\n",
        "def run(line):\n",
        "  with open(op_csv+countStr+(\".csv\"), newline='') as f:\n",
        "    lines = f.readlines()\n",
        "  chunks = [' '.join(line[i:i+100]) for i in range(0, len(line), 100)]\n",
        "  taken=[]\n",
        "  sl=stopwords_text()\n",
        "  for tok in chunks:\n",
        "    # rmPOS=preprocessPOS(tok)\n",
        "    # text = preprocess(rmPOS)\n",
        "    # doc = create_doc(text)\n",
        "\n",
        "    rmPOS=\"\".join(tok)\n",
        "    # text = preprocess(rmPOS)\n",
        "    doc = create_doc(rmPOS)\n",
        "\n",
        "    for tok in doc.ents:\n",
        "      if tok.text:\n",
        "        print(tok.text,\"   \",tok.label_)\n",
        "        taken.append(tok.text)\n",
        "\n",
        "  print(set(taken))\n",
        "\n",
        "  # sl=stopwords_text()\n",
        "  # result=[]\n",
        "\n",
        "  # for token in x.split( ):\n",
        "  #   # print(token)\n",
        "  #   if not token in sl:\n",
        "  #     result.append(token)\n",
        "  # w=set(result)\n",
        "  # strResult=  \" \".join(w)# list -> str \n",
        "  # # print(strResult)"
      ],
      "metadata": {
        "id": "4IyrMFWo3HQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(lines)"
      ],
      "metadata": {
        "id": "6pLI1m1y3HN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"GPE\")#xpacyのラベル確認"
      ],
      "metadata": {
        "id": "PUrXeeu03HLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58xpAQtJ3HI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQx1yKF73HHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}