{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textRank_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO7SkSxfol1cTPNiMQIapR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subhutii/GRIT-AI/blob/main/textRank_ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QrQNCGQYyRT"
      },
      "outputs": [],
      "source": [
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from textacy import extract, make_spacy_doc\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.ja.examples import sentences \n",
        "\n",
        "#https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part5.html\n"
      ],
      "metadata": {
        "id": "i284-U80ZT06"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neologdn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUpLnXXldlAJ",
        "outputId": "a694157e-03a2-4ea8-fa58-ab32ee6e27b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neologdn\n",
            "  Downloading neologdn-0.5.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.5.1-cp37-cp37m-linux_x86_64.whl size=172961 sha256=60d0a73660cec59e16355f5d969356378091cab7b97311ce35ad8d6a5d1fd0b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/15/5c/55b33d02e16129ef81313e4c86e473d6dd1cecf7317a525a9b\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "vTn3XvIanaOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import neologdn\n",
        "import re\n",
        "import string\n",
        "from typing import List, Tuple\n",
        "import spacy\n",
        "from spacy.lang.ja.examples import sentences\n",
        "from textacy import extract\n"
      ],
      "metadata": {
        "id": "RrL8LXsJddOo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テキスト取得\n",
        "with open(\"text_ja.txt\", \"r\") as file:\n",
        "    data = file.read().replace(\"\\n\", \"\")\n",
        "article = data.replace(u\"\\xa0\", u\" \") # ノーブレークスペース削除\n"
      ],
      "metadata": {
        "id": "hy92lenQUslR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NKSFTHJ0XQx",
        "outputId": "bb25dcb5-e397-47b7-f19e-6c698d01066f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function print>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ストップワード処理\n"
      ],
      "metadata": {
        "id": "5JCv7SGxoxYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "def preprocess(x):\n",
        "  #絵文字の除去\n",
        "  emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"]+\",\n",
        "            flags=re.UNICODE,\n",
        "        )\n",
        "  x = emoji_pattern.sub(r\"\", x)\n",
        "\n",
        "  x = neologdn.normalize(x)\n",
        "  x = x.lower()\n",
        "  x = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", x)#URLの除去\n",
        "  x = re.sub(r\"[!-/:-@[-`{-~]\", r\" \", x)# 半角記号の置換\n",
        "  x = re.sub(\"[■-♯]\", \" \", x)# 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
        "  # x = re.sub(r\"(\\d)([,.])(\\d+)\", \"\\1\\3\", x)#桁区切り (+ 小数点) 除去\n",
        "  # x = re.sub(r\"\\d+\", \"0\", x)#数字を0に置換\n",
        "  # x = re.sub(r\"・\", \", \", x)\n",
        "  x = re.sub(r\"[\\(\\)「」【】]\", \"\", x)# 括弧削除（まだ消すやつある）\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "RN_5JknajFMR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy(textacy)+textrank"
      ],
      "metadata": {
        "id": "IXaqpxTppIfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract後の可視化\n",
        "def decompose_keyterms(keyterm_list: List[str]) -> Tuple:#list [], tuple ()  変更ができるのがリスト。変更ができないのがタプル\n",
        "#keyterm_listをelに入れて el[0].replace(\"\\n\", \" \") で置き換えたもの（改行を\"\"に置き換え，0番目の要素）をtermsに入れる\n",
        "    # terms=[]\n",
        "    # for el in keyterm_list:\n",
        "    #     terms.append(el[0])\n",
        "    \n",
        "    terms = [el[0] for el in keyterm_list]\n",
        "    # terms = [el[0].replace(\"\\n\", \" \") for el in keyterm_list]\n",
        "    scores = [el[1] for el in keyterm_list]\n",
        "    # scores = np.asarray([el[1] for el in keyterm_list])\n",
        "    return terms, scores\n",
        "\n",
        "#トークン化\n",
        "def create_doc(article):\n",
        "    texts = article.split(',')\n",
        "    article_1 = \"\"\n",
        "    for text in texts:\n",
        "      article_1+=text+\",\"\n",
        "\n",
        "    # doc = make_spacy_doc(article_1, lang=\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"ja_core_news_sm\")\n",
        "    doc = nlp(article_1)\n",
        "    return doc\n",
        "\n",
        "#textrank\n",
        "def textrank_algo(doc):\n",
        "    textrank = extract.keyterms.textrank(doc, normalize=\"lemma\")\n",
        "    terms_textrank, scores_textrank = decompose_keyterms(textrank)\n",
        "\n",
        "    return terms_textrank, scores_textrank"
      ],
      "metadata": {
        "id": "aCLIjnTqik5V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yake\n",
        "def yake_algo(doc):\n",
        "    yake = extract.keyterms.yake(doc, normalize=\"lemma\")\n",
        "    terms_yake, scores_yake = decompose_keyterms(yake)\n",
        "\n",
        "    return terms_yake, scores_yake\n",
        "\n",
        "#scake\n",
        "def scake_algo(doc):\n",
        "    scake = extract.keyterms.scake(doc, normalize=\"lemma\")\n",
        "    terms_scake, scores_scake = decompose_keyterms(scake)\n",
        "\n",
        "    return terms_scake, scores_scake\n",
        "\n",
        "#sgrank\n",
        "def sgrank_algo(doc):\n",
        "    sgrank = extract.keyterms.sgrank(doc, normalize=\"lemma\")\n",
        "    terms_sgrank, scores_sgrank = decompose_keyterms(sgrank)\n",
        "    return terms_sgrank, scores_sgrank"
      ],
      "metadata": {
        "id": "huQ1EH2cooym"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj8h4CV4oFzG",
        "outputId": "be2aff6e-6f1e-4570-d2c1-33cd58338fc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#呼び出し\n",
        "def main(article):\n",
        "  #part1\n",
        "    text = preprocess(article) #ストップワード削除\n",
        "    doc = create_doc(text)\n",
        "\n",
        "#part2\n",
        "    # doc = create_doc(article)\n",
        "\n",
        "    terms_textrank, score_textrank = textrank_algo(doc)\n",
        "    # terms_yake, score_yake = yake_algo(doc)\n",
        "    # terms_scake, score_scake = scake_algo(doc)\n",
        "    # terms_sgrank, scores_sgrank = sgrank_algo(doc)\n",
        "\n",
        "    print(\"textrank\", terms_textrank)\n",
        "    # print(\"textrank score\", score_textrank)\n",
        "    # print(\"yake\", terms_yake)\n",
        "    # print(\"scake\", terms_scake)\n",
        "    # print(\"sgrank\", terms_sgrank)\n",
        "    print(type(terms_textrank))\n",
        "\n",
        "    #品詞表示\n",
        "    # POS_tag = nltk.pos_tag(terms_textrank)\n",
        "    # print(POS_tag)\n",
        "\n",
        "main(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMIRtRgvlXGe",
        "outputId": "97c9c136-4306-4c77-d3f2-237ce4e87b34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textrank ['富士 山 本宮 浅間 大社', '笑 富士 山', '富士 山 パーキング', '合 目 着', '難しい 山', '山 小屋', '合 目 あたり', '問題 ない', '富士 急行 バス', '持ち主 徳川 家康']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFBbtL0tnSE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}